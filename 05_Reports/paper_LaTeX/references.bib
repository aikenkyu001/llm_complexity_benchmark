@article{qwen2024,
  title={Qwen2.5-Coder Technical Report},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@article{gemma2025,
  title={Gemma 3: Hybrid Modality and Scalable Intelligence},
  author={Google DeepMind},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}

@article{humaneval2021,
  title={Measuring Coding Challenge Competence With HumanEval},
  author={Chen, Mark and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{mbpp2021,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@inproceedings{lora2021,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and others},
  booktitle={Proceedings of the 2022 International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{modelmerge2024,
  title={Model Merging: A Survey},
  author={Yadav, Prateek and others},
  journal={arXiv preprint arXiv:2403.13257},
  year={2024}
}

@article{selfcorrect2023,
  title={Large Language Models Cannot Self-Correct},
  author={Huang, Jie and others},
  journal={arXiv preprint arXiv:2309.06275},
  year={2023}
}

@article{phi22024,
  title={Phi-2: The Surprising Power of Small Language Models},
  author={Gunasekar, Suriya and others},
  journal={arXiv preprint arXiv:2402.14020},
  year={2024}
}

@article{gemma2024,
  title={Gemma: Open Models Based on Gemini Research},
  author={Google Team},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{cot2022,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and others},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}
